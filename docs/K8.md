# Kubernetes and other fcks

## Day 1

```bash
helm install hello-redis bitnami/redis
helm uninstall hello-redis

helm repo update
helm repo add bitnami <bitnami>
```

## kubeapps

```bash
kubectl create ns kubeapps
kubens kubeapps
helm install kubeapps --namespace kubeapps bitnami/kubeapps

kubectl create serviceaccount kubeapps-operator
kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator

# -- namespace is important here much
kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='{range .secrets[*]}{.name}{"\n"}{end}' | grep kubeapps-operator-token) -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}' && echo

```

## Kafka

- https://docs.confluent.io/5.1.2/installation/installing_cp/cp-helm-charts/docs/index.html
- https://github.com/confluentinc/cp-helm-charts/blob/master/values.yaml

```bash
kubectl create ns kafka

helm install confluentinc/cp-helm-charts --name kafka-one
helm install --set cp-schema-registry.enabled=false,cp-kafka-rest.enabled=false,cp-kafka-connect.enabled=false confluentinc/cp-helm-charts
```

---
NAME: kafka-one
LAST DEPLOYED: Mon Nov  2 15:27:47 2020
NAMESPACE: kafka
STATUS: deployed
REVISION: 1
NOTES:

### Zookeeper

Connection string for Confluent Kafka:
  kafka-one-cp-zookeeper-0.kafka-one-cp-zookeeper-headless:2181,kafka-one-cp-zookeeper-1.kafka-one-cp-zookeeper-headless:2181,...

To connect from a client pod:

1. Deploy a zookeeper client pod with configuration:

    apiVersion: v1
    kind: Pod
    metadata:
      name: zookeeper-client
      namespace: kafka
    spec:
      containers:
      - name: zookeeper-client
        image: confluentinc/cp-zookeeper:5.5.0
        command:
          - sh
          - -c
          - "exec tail -f /dev/null"

2. Log into the Pod

  kubectl exec -it zookeeper-client -- /bin/bash

3. Use zookeeper-shell to connect in the zookeeper-client Pod:

  zookeeper-shell kafka-one-cp-zookeeper:2181

4. Explore with zookeeper commands, for example:

  Gives the list of active brokers
  ls /brokers/ids

  Gives the list of topics
  ls /brokers/topics

  Gives more detailed information of the broker id '0'
  get /brokers/ids/0## ------------------------------------------------------

### Kafka

To connect from a client pod:

1. Deploy a kafka client pod with configuration:

    apiVersion: v1
    kind: Pod
    metadata:
      name: kafka-client
      namespace: kafka
    spec:
      containers:
      - name: kafka-client
        image: confluentinc/cp-enterprise-kafka:5.5.0
        command:
          - sh
          - -c
          - "exec tail -f /dev/null"

2. Log into the Pod

  kubectl exec -it kafka-client -- /bin/bash

3. Explore with kafka commands:

  Create the topic
  kafka-topics --zookeeper kafka-one-cp-zookeeper-headless:2181 --topic kafka-one-topic --create --partitions 1 --replication-factor 1 --if-not-exists

  Create a message
  MESSAGE="`date -u`"

  Produce a test message to the topic
  echo "$MESSAGE" | kafka-console-producer --broker-list kafka-one-cp-kafka-headless:9092 --topic kafka-one-topic

  Consume a test message from the topic
  kafka-console-consumer --bootstrap-server kafka-one-cp-kafka-headless:9092 --topic kafka-one-topic --from-beginning --timeout-ms 2000 --max-messages 1 | grep "$MESSAGE"

---

## telepresence
```bash
telepresence --run-shell
```

## kafkacat

```bash
kafkacat -b kafka-one-cp-kafka-headless:9092 -o beginning -t kafka-one-topic
kafkacat -P -b kafka-one-cp-kafka-headless:9092 -o beginning -t kafka-one-topic
```

## connect connectors

```bash
docker run -ti --rm 
    -e KAFKA_MQTT_BOOTSTRAP_SERVERS kafka-one-cp-kafka-headless:9092
    confluentinc/cp-kafka-mqtt:latest
```

- https://enfuse.io/a-diy-guide-to-kafka-connectors/

## Neo4j

```bash
helm install neo-one RELEASE_URL \
    --set core.standalone=true \
    --set acceptLicenseAgreement=yes \
    --set neo4jPassword=mySecretPassword`
```
l    
NAME: neo-one
LAST DEPLOYED: Mon Nov  2 23:59:16 2020
NAMESPACE: neo4j
STATUS: deployed
REVISION: 1
NOTES:
Your cluster is now being deployed, and may take up to 5 minutes to become available.
If you'd like to track status and wait on your rollout to complete, run:

$ kubectl rollout status \
    --namespace neo4j \
    StatefulSet/neo-one-neo4j-core \
    --watch

You can inspect your logs containers like so:

We can see the content of the logs by running the following command:

$ kubectl logs --namespace neo4j -l \
    "app.kubernetes.io/instance=neo-one,app.kubernetes.io/name=neo4j,app.kubernetes.io/component=core"

We can now run a query to find the topology of the cluster.

export NEO4J_PASSWORD=$(kubectl get secrets neo-one-neo4j-secrets --namespace neo4j -o yaml | grep password | sed 's/.*: //' | base64 -d)
kubectl run -it --rm cypher-shell \
    --image=neo4j:4.1.3-enterprise \
    --restart=Never \
    --namespace neo4j \
    --command -- ./bin/cypher-shell -u neo4j -p "$NEO4J_PASSWORD" -a neo4j://neo-one-neo4j.neo4j.svc.cluster.local "call dbms.routing.getRoutingTable({}, 'system');"

This will print out the addresses of the members of the cluster.

Note:
You'll need to substitute <password> with the password you set when installing the Helm package.
If you didn't set a password, one will be auto generated.
You can find the base64 encoded version of the password by running the following command:

kubectl get secrets neo-one-neo4j-secrets -o yaml --namespace neo4j    

## ElasticSearch

```bash
helm install es-one elastic/elasticsearch
kubectl get pods --namespace=kafka -l app=elasticsearch-master -w
curl -s  elasticsearch-master:9200/_cluster/state | jq
```